{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    TextDataset,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "\n",
    "# from trl import SFTTrainer\n",
    "import gc\n",
    "import json\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.empty_cache()\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "from config import *\n",
    "import itertools\n",
    "\n",
    "eval_dataset_json_file = \"/home/admin/blessed/questions_new_final.json\"\n",
    "output_dir = \"/home/admin/blessed/home/ubuntu/checkpoint-1400\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.42s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"checkpoint-2700\", torch_dtype=\"auto\", trust_remote_code=True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir, trust_remote_code=True)\n",
    "# tokenizer.pad_token=tokenizer.eos_token\n",
    "# tokenizer.padding_side=\"left\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load base moodel\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    output_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map={\"\": 0},\n",
    "    # revision=\"refs/pr/23\" #the main version of Phi-2 doesn’t support gradient checkpointing (while training this model)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(eval_dataset_json_file, \"r\") as f:\n",
    "    test_data = f.read()\n",
    "\n",
    "orig_test_data=json.loads(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_options  = []\n",
    "def fetch_question(example):\n",
    "    global all_options\n",
    "    data_ = {\"questions\": []}\n",
    "\n",
    "\n",
    "    for ex in example:\n",
    "\n",
    "        options = \"\\n\"\n",
    "        opts = []\n",
    "        for key in example[ex].keys():\n",
    "            if key.startswith(\"opt\"):\n",
    "                if example[ex][key] == None:\n",
    "                    continue\n",
    "                options += str(key) + \" \" +example[ex][key]+\"\\n\"\n",
    "                opts.append((example[ex][key], key.split(\"option \")[1]))\n",
    "\n",
    "        all_options.append(opts)\n",
    "\n",
    "\n",
    "        q = \"Instruct:\"+\"\\n\"+example[ex]['question'] + \"\\n\\n\\nAbbreviations: \\n\"   +\\\n",
    "            '\\n'.join(str(e) for e in  list(dict.fromkeys(example[ex][\"abbreviation\"])) )\\\n",
    "            +\"\\n\\nConsidering the following retrieved contexts\"+\"\\ncontext 1: \"+example[ex][\"context_qwen2\"][0]+example[ex][\"context_qwen2\"][1]+\"\\ncontext 2: \"+'\\n'.join(example[ex][\"context_gle\"] ) + \"\\ncontext 3: \"+example[ex][\"context_bm\"][0] +\"\\n\"+ example[ex]['question'] + \"\\n\" + options\n",
    "\n",
    "\n",
    "        q += \"\\nOutput:\" \n",
    "\n",
    "        #   ans = example[ex][\"answer\"] + \"\\n ### Answer: \" + example[ex][\"explanation\"]\n",
    "\n",
    "        data_[\"questions\"].append(q)\n",
    "        \n",
    "    return data_\n",
    "eval_dataset  = fetch_question(orig_test_data)\n",
    "\n",
    "\n",
    "all_test = []\n",
    "all_test_ids = []\n",
    "prompt =\"Instruct: Answer the following question using the context provided.Your answer must start with the correct option letter (A, B, C, D, or E) followed by the text of the answer.\"\n",
    "\n",
    "\n",
    "prompt =\"Instruct: Answer the following question using the context provided.Your answer must start with the correct option letter (A, B, C, D, or E) followed by the text of the answer.\"\n",
    "for key in (orig_test_data):\n",
    "\n",
    "  all_test_ids.append(int(key.split(' ')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from statistics import mode\n",
    "submission = {\"Question_ID\": [], \"Answer_ID\": []}\n",
    "batch_size = 1 # Adjust batch size as needed\n",
    "\n",
    "options_ids = [\"option 1\", \"option 2\", \"option 3\", \"option 4\", \"option 5\"]\n",
    "if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "# Set the model to evaluation mode once\n",
    "model.eval()\n",
    "# Group data into batches\n",
    "bad_ids = []\n",
    "for i in range(0, len(eval_dataset['questions']), batch_size):\n",
    "\n",
    "    batch_question_ids = all_test_ids[i:i + batch_size]\n",
    "\n",
    "    batch_prompts = eval_dataset['questions'][i:i + batch_size]\n",
    "    string_opts = ' '.join(opt[0] for opt in all_options[i])\n",
    "    # print(string_opts)\n",
    "    if not (\"option\" in string_opts or \"above\" in string_opts):\n",
    "        print(\"batch shuffle\")\n",
    "        # Generate all permutations of the list\n",
    "        votes = []\n",
    "        batch_prompts = []\n",
    "        all_permutations = list(itertools.permutations(all_options[i]))\n",
    "        all_permutations = random.sample(all_permutations, 20 if len(all_permutations)>20 else len(all_permutations))\n",
    "        for option_set in all_permutations:\n",
    "                option_map = []\n",
    "                options = \"\\n\"\n",
    "               \n",
    "                for z in range(len(option_set)):\n",
    "                    options += options_ids[z] + \" \" +  option_set[z][0] + \"\\n\"\n",
    "                    option_map.append(option_set[z][1])\n",
    "\n",
    "                batch_prompts.append(question_former(\"question \"+str(all_test_ids[i]), options))\n",
    "\n",
    "        model_inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=False,padding_side =\"left\" truncation=True).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            results = model.generate(**model_inputs, pad_token_id=tokenizer.pad_token_id, max_new_tokens=5,)\n",
    "        \n",
    "        for file \n",
    "            batch_results = [tokenizer.decode(result, skip_special_tokens=True) for result in results]\n",
    "            answer_id = int(batch_results[0].split(\"Output:\")[1].split(\"option\")[1][1])\n",
    "            votes.append(option_map[answer_id-1])\n",
    "            \n",
    "            \n",
    "            \n",
    "            answer_id = int(mode(votes))\n",
    "        submission[\"Answer_ID\"].append(answer_id)\n",
    "        submission[\"Question_ID\"].append(batch_question_ids[0])\n",
    "        \n",
    "    else:\n",
    "\n",
    "        batch_question_ids = all_test_ids[i:i + batch_size]\n",
    "        # Tokenize the batch prompts\n",
    "        model_inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=False, truncation=True).to(\"cuda\")\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Perform batch inference\n",
    "        with torch.no_grad():\n",
    "            results = model.generate(**model_inputs, pad_token_id=tokenizer.pad_token_id, max_new_tokens=5,)\n",
    "\n",
    "        # Decode the batch results\n",
    "        batch_results = [tokenizer.decode(result, skip_special_tokens=True) for result in results]\n",
    "    \n",
    "        # print(time.time() - start)\n",
    "        # print(batch_results[0])\n",
    "        \n",
    "        # Extract the answer IDs from batch results\n",
    "        for question_id, result_text in zip(batch_question_ids, batch_results):\n",
    "\n",
    "            submission[\"Question_ID\"].append(question_id)\n",
    "            try:\n",
    "                answer_id = int(result_text.lower().split(\"Output:\")[1].split(\"option\")[1][1])\n",
    "                submission[\"Answer_ID\"].append(answer_id)\n",
    "            except:\n",
    "                try:\n",
    "                \n",
    "                    answer_id = (result_text.lower().split(\"output:\")[2].split(\"option\")[1][1])\n",
    "\n",
    "                    submission[\"Answer_ID\"].append(int(answer_id))\n",
    "                except:\n",
    "                    print(result_text)\n",
    "                    submission[\"Answer_ID\"].append(random.randint(1, 4))\n",
    "               \n",
    "    \n",
    "    print(i, answer_id)\n",
    "    \n",
    "# # # Create a DataFrame and save it to a CSV file\n",
    "df_submission = pd.DataFrame(submission)\n",
    "df_submission.to_csv(\"submission_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(submission[\"Answer_ID\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(len(submission[\"Question_ID\"])))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(submission)\n",
    "df_submission.to_csv(\"submission_test_halfway.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"Answer_ID\"].append(2)\n",
    "submission[\"Question_ID\"].append(batch_question_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_question_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(submission[\"Answer_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_results[0].split(\"Output:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mode(votes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(submission[\"Answer_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20 if len(all_permutations)>20 else len(all_permutations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(submission[\"Answer_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(submission)\n",
    "df_submission.to_csv(\"submission_full_batch_trick.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(submission[\"Answer_ID\"][:100] == answers_df[\"Answer_ID\"][:100])/len(submission[\"Answer_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(submission[\"Answer_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_best = pd.read_csv(\"new_best.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(new_best[\"Answer_ID\"][:100] == answers_df[\"Answer_ID\"][:100])/len(submission[\"Answer_ID\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    " \n",
    "def most_common(List):\n",
    "    return(mode(List))\n",
    "\n",
    "type(mode(votes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(option_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option_set[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_permutations[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def fetch_question(example):\n",
    "#     data_ = {\"questions\": []}\n",
    "\n",
    "#     for ex in example:\n",
    "#     #   print(len( example[ex]['question'].split(\"Please provide a detailed answer to the following question by starting with mentioning the correct option:\")))\n",
    "#         question  = example[ex][\"question\"]\n",
    "#         # print(question, example[ex].keys())\n",
    "#         options = \"\\n\"\n",
    "#         for key in example[ex].keys():\n",
    "#            if key.startswith(\"opt\"):\n",
    "#               if example[ex][key] == None:\n",
    "#                  continue\n",
    "#               options += str(key) + \" \" +example[ex][key]+\"\\n\"\n",
    "\n",
    "#         q = \"Instruct:Answer the following Telecommunication standards question considering the context provided, if the answer is not in the context, use your own Telecommunication/Networking knowledge to answer. :\\n \"+ question+ \"\\nAbbreviations: \\n\" +\\\n",
    "#             '\\n'.join(str(e) for e in  list(dict.fromkeys(example[ex][\"abbreviation\"])) )\\\n",
    "#             +\"\\n\\nConsidering the following retrieved contexts\"+\"\\ncontext 1: \"+ \"\\n\".join(example[ex][\"context_bm_300\"])+ \"\\ncontext 2:\"+'\\n: '.join(example[ex][\"context_gle\"])+\"\\ncontext 3:\"+'\\n'.join(example[ex][\"context_gte\"][:4]) +\"\\n\" + question + options\n",
    "#             # +\"\\n\\nConsidering the following retrieved contexts\"+\"\\ncontext 1: \"+example[ex][\"context_gte\"][0]+\"\\ncontext 2: \"+'\\n'.join(example[ex][\"context_gle\"]) + \"\\ncontext 3: \"+example[ex][\"context_bm\"][0] +\"\\n\"+ question + options\n",
    "        \n",
    "\n",
    "\n",
    "#         q += \"\\nOutput:\" \n",
    "\n",
    "#         #   ans = example[ex][\"answer\"] + \"\\n ### Answer: \" + example[ex][\"explanation\"]\n",
    "\n",
    "#         data_[\"questions\"].append(q)\n",
    "        \n",
    "#     return data_\n",
    "# eval_dataset  = fetch_question(orig_test_data)\n",
    "# # eval_dataset = Dataset.from_dict({\n",
    "# #     'text': eval_dataset['questions'],\n",
    "# # })\n",
    "\n",
    "\n",
    "# all_test = []\n",
    "# all_test_ids = []\n",
    "\n",
    "# for key in (orig_test_data):\n",
    "#   all_test_ids.append(int(key.split(' ')[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_dataset[\"questions\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "submission = {\"Question_ID\": [], \"Answer_ID\": []}\n",
    "batch_size = 1 # Adjust batch size as needed\n",
    "\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "# Set the model to evaluation mode once\n",
    "model.eval()\n",
    "\n",
    "# Group data into batches\n",
    "bad_ids = []\n",
    "for i in range( 0, len(eval_dataset['questions']), batch_size):\n",
    "    # batch_prompts = questions[i:i + batch_size]\n",
    "    # batch_prompts = [raw_json['train'][key][0] + \"\\n ### Answer: \" for key in batch_questions]\n",
    "    # batch_question_ids = [int(key.split(\" \")[1]) for key in batch_questions]\n",
    "\n",
    "    batch_prompts = eval_dataset['questions'][i:i + batch_size]\n",
    "    batch_question_ids = all_test_ids[i:i + batch_size]\n",
    "    # Tokenize the batch prompts\n",
    "    model_inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=False, truncation=True).to(\"cuda\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Perform batch inference\n",
    "    with torch.no_grad():\n",
    "        results = model.generate(**model_inputs, pad_token_id=tokenizer.pad_token_id, max_new_tokens=5,)\n",
    "\n",
    "    # Decode the batch results\n",
    "    batch_results = [tokenizer.decode(result, skip_special_tokens=True) for result in results]\n",
    "   \n",
    "    # print(time.time() - start)\n",
    "    # print(batch_results[0])\n",
    "    \n",
    "    # Extract the answer IDs from batch results\n",
    "    for question_id, result_text in zip(batch_question_ids, batch_results):\n",
    "        # try:\n",
    "      \n",
    "        # except:\n",
    "        #     print(result_text.split(\"Output:\")[1])\n",
    "        #     answer_id = random.randint(1,5)\n",
    "        submission[\"Question_ID\"].append(question_id)\n",
    "        try:\n",
    "            answer_id = int(result_text.split(\"Output:\")[1].split(\"option\")[1][1])\n",
    "            submission[\"Answer_ID\"].append(answer_id)\n",
    "        except:\n",
    "            try:\n",
    "            \n",
    "                answer_id = (result_text.lower().split(\"output:\")[2].split(\"option\")[1][1])\n",
    "\n",
    "                submission[\"Answer_ID\"].append(int(answer_id))\n",
    "            except:\n",
    "                 print(result_text)\n",
    "                 submission[\"Answer_ID\"].append(random.randint(1, 4))\n",
    "                 bad_ids.append(i)\n",
    "    if i == 365:\n",
    "         break\n",
    "    \n",
    "    print(i, answer_id)\n",
    "    \n",
    "# # Create a DataFrame and save it to a CSV file\n",
    "df_submission = pd.DataFrame(submission)\n",
    "df_submission.to_csv(\"submission_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load the CSV files into DataFrames\n",
    "answers_df = pd.read_csv('answers.csv')\n",
    "submission_df = pd.read_csv('submission_test.csv')\n",
    "print(len(answers_df), len(submission_df))\n",
    "# Merge the DataFrames on the 'Question_ID' column\n",
    "merged_df = pd.merge(answers_df, submission_df, on='Question_ID', suffixes=('_correct', '_submitted'))\n",
    "\n",
    "# Calculate the number of correct answers\n",
    "correct_answers = (merged_df['Answer_ID_correct'] == merged_df['Answer_ID_submitted']).sum()\n",
    "\n",
    "# Calculate the total number of questions\n",
    "total_questions = len(merged_df)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_answers / total_questions\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_ids[365]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['Answer_ID_correct'] != merged_df['Answer_ID_submitted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"Answer_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(submission[\"Answer_ID\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
