{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "prompt_q_without_contex_train= Template('''Instruct: $question\n",
    "$options\n",
    "$question\n",
    "''')\n",
    "\n",
    "\n",
    "prompt_without_contex_train= Template('''Instruct: $question\n",
    "Abbreviations: $abbreviation\n",
    "          \n",
    "Considering the following contexts:\n",
    "context 1: $context1\n",
    "context 2: $context2\n",
    "context 3: $context3      \n",
    "                                                                    \n",
    "$question\n",
    "$options\n",
    "Output: option ''')\n",
    "# prompt_without_context = f'Hello {planet}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_question(question):\n",
    "    for num in [14, 15, 16, 17, 18]:\n",
    "        question = question.replace(f\"[3GPP Release {num}]\", \"\")\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "BASE_MODEL_ID = \"microsoft/phi-2\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "# model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLLMDataloader:\n",
    "    def __init__(self, batch_size, tokenizer, data, shuffle = False, val= False):\n",
    "        ## initializations\n",
    "        self.batch_size  = batch_size\n",
    "        self.tokenizer  = tokenizer\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        with open(data, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "        self.all_examples = list(self.data.keys())\n",
    "        self.shuffle = shuffle\n",
    "        self.val = val\n",
    "        \n",
    "        self.n_data_points = math.ceil(len(self.data)/self.batch_size)\n",
    "        self.indices = [i for i in range(self.n_data_points)]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        ## this gets a batch \n",
    "        option_header = [\"option 1 \", \"option 2 \", \"option 3 \", \"option 4 \", \"option 5 \"]\n",
    "        batch_start_id = idx * self.batch_size\n",
    "        batch_end_id  = min(len(self.data), batch_start_id + self.batch_size) \n",
    "        batch = {\"question_context\":[], \"answer\":[]}\n",
    "        \n",
    "        for i in range(batch_start_id, batch_end_id):\n",
    "            example = self.data[self.all_examples[i]]\n",
    "            options = []\n",
    "            for key in example.keys():\n",
    "                if key.startswith(\"opt\"):\n",
    "                    options.append(example[key])\n",
    "\n",
    "\n",
    "            correct_option_txt =example[\"answer\"][10:]\n",
    "            correct_option_id = int(example[\"answer\"].split(\"option \")[1][0])  - 1\n",
    "           \n",
    "\n",
    "            if self.shuffle:\n",
    "                random.shuffle(options)\n",
    "                correct_option_id = options.index(correct_option_txt) +1\n",
    "            \n",
    "                options_with_header = [option_header[i] +options[i] for i in range(len(options)) ]\n",
    "                options_with_header = \"\\n\".join(options_with_header)\n",
    "                # correct_option_txt = example[\"answer\"].split(\": \")[1]\n",
    "                correct_option_txt = example[\"answer\"][10:]\n",
    "\n",
    "                correct_option_txt_header = str(correct_option_id) +\" \" + correct_option_txt\n",
    "\n",
    "                prompt = prompt_without_contex_train.substitute(question = clean_question(example[\"question\"]),\\\n",
    "                abbreviation='\\n'.join(example[\"abbreviation\"]), context1 = '\\n'.join(example[\"context_qwen2\"][:2]) , context2 = '\\n'.join(example[\"context_gle\"]), context3 = '\\n'.join(example[\"context_bm\"][:2]),\n",
    "                options =options_with_header)\n",
    "\n",
    "            else:\n",
    "                correct_option_id = correct_option_id+1\n",
    "                correct_option_txt_header = str(correct_option_id) +\" \" + correct_option_txt\n",
    "                options_with_header = [option_header[i] +options[i] for i in range(len(options)) ]\n",
    "                options_with_header = \"\\n\".join(options_with_header)\n",
    "                prompt = prompt_without_contex_train.substitute(question = clean_question(example[\"question\"]),\\\n",
    "                abbreviation='\\n'.join(example[\"abbreviation\"]), context1 = '\\n'.join(example[\"context_qwen2\"][:2]) , context2 = '\\n'.join(example[\"context_gle\"]), context3 = '\\n'.join(example[\"context_bm\"][:2]),\n",
    "                options =options_with_header)\n",
    "            ## context TBD\n",
    "            # answer =  f\"{correct_option_txt_header}  \\nExplanation: {example['explanation']}\"\n",
    "            if not self.val:\n",
    "                answer =  f\"{correct_option_txt_header}  \\nExplanation: {example['explanation']}\"\n",
    "            else:\n",
    "                answer =  f\"{correct_option_txt_header}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            batch[\"question_context\"] += [prompt]\n",
    "\n",
    "            batch[\"answer\"] += [answer]\n",
    "\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "        q_tokens = self.tokenizer(batch[\"question_context\"], padding=\"longest\", return_tensors=\"pt\")  \n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "        a_tokens = self.tokenizer(batch[\"answer\"], padding=\"longest\", return_tensors=\"pt\")\n",
    "        tokens = torch.cat([q_tokens[\"input_ids\"], a_tokens[\"input_ids\"]], dim=1)\n",
    "        attn_masks = torch.cat([q_tokens[\"attention_mask\"], a_tokens[\"attention_mask\"]], dim=1)\n",
    "        loss_mask = torch.cat([torch.zeros_like(q_tokens[\"attention_mask\"]), a_tokens[\"attention_mask\"]], dim=1)[:,1:]\n",
    "   \n",
    "        result = {\n",
    "        \"inp_ids\":tokens[:,:-1],\n",
    "        \"inp_mask\":attn_masks[:,:-1],## Causal Training\n",
    "        \"out_ids\":tokens[:,1:], ## Causal Labels\n",
    "        \"out_mask\":attn_masks[:,1:],\n",
    "        \"q_tokens\": q_tokens,\n",
    "        \"a_tokens\": a_tokens,}\n",
    "        result[\"loss_mask\"] = loss_mask * result[\"out_mask\"]\n",
    "        # result[\"out_ids\"][:,:q_tokens[\"input_ids\"].size(1)-10] = self.tokenizer.eos_token_id\n",
    "\n",
    "        return result       \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    def __iter__(self):\n",
    "        self.idx = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.idx >= self.n_data_points:\n",
    "            self.idx = 0\n",
    "            raise StopIteration\n",
    "        temp_idx = self.indices[self.idx]\n",
    "        self.idx += 1\n",
    "        return self[temp_idx]\n",
    "             \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_data_points\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, batch):\n",
    "    inp_ids = batch[\"inp_ids\"].to(model.device)\n",
    "    attn_mask = batch[\"inp_mask\"].to(model.device)\n",
    "    result = model(input_ids=inp_ids, attention_mask=attn_mask)\n",
    "    logits = result.logits\n",
    "    return logits\n",
    "\n",
    "\n",
    "def calc_loss(loss_fn, logits, batch):\n",
    "    B, L, C = logits.shape\n",
    "    target = batch[\"out_ids\"].to(logits.device)\n",
    "    mask = batch[\"loss_mask\"].to(logits.device)\n",
    "    loss = loss_fn(logits.reshape(-1, C), target.reshape(-1)) * mask.reshape(-1)\n",
    "    loss = loss.sum()/mask.sum()\n",
    "    return loss\n",
    "\n",
    "def update(model, optimizer, loss_fn, batch, accumulate_grad=True):\n",
    "    with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "        logits = forward_pass(model, batch)\n",
    "        loss = calc_loss(loss_fn, logits, batch)\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "    if not accumulate_grad:\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "    return loss.item(), logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_on_batches(model, train_data_loader, optimizer, loss_fn, grad_accum_bs=16):\n",
    "    train_loss = 0\n",
    "    score = 0\n",
    "    num_correct = 0\n",
    "    total_num = 0\n",
    "    pbar = tqdm(range(len(train_data_loader)), ncols=100)\n",
    "    option_ids = [tokenizer(o).input_ids[0] for o in [\"1\", \"2\", \"3\", \"4\", \"5\"]]\n",
    "    model.train()\n",
    "    for i,batch in enumerate(train_data_loader):\n",
    "        grad_step_bs = grad_accum_bs/train_data_loader.batch_size\n",
    "        if (i>=grad_step_bs and i%grad_step_bs == 0) or i == len(train_data_loader)-1:\n",
    "            accumulate_grad = False\n",
    "        else:\n",
    "            accumulate_grad = True\n",
    "        loss, logits = update(model, optimizer, loss_fn, batch, accumulate_grad=accumulate_grad)\n",
    "        train_loss += (1/(i+1))*(loss-train_loss)\n",
    "        # if i% 50 == 0:\n",
    "        #     print(logits.shape, logits[:,batch[\"q_tokens\"].input_ids.size(1)-1,option_ids].shape, logits[ :,batch[\"q_tokens\"].input_ids.size(1)-1:  ].shape, train_data_loader.tokenizer.batch_decode(logits[ :,batch[\"q_tokens\"].input_ids.size(1)-1:  ].argmax(dim=2)))\n",
    "        pred = (logits[:,batch[\"q_tokens\"].input_ids.size(1)-1,option_ids].argmax(dim=1) + 1).tolist()\n",
    "        target = train_data_loader.tokenizer.batch_decode(batch[\"a_tokens\"].input_ids[:,0], skip_special_tokens=True)\n",
    "        for p,t in zip(pred, target):\n",
    "            total_num += 1\n",
    "            num_correct += (int(p) == int(t))\n",
    "            score = num_correct/total_num\n",
    "        \n",
    "\n",
    "        pbar.set_description(f\"Train Loss: {train_loss:.4f} Score: {score:.4f}\")\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    return train_loss, score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tdataloader,vdataloader, optimizer, loss_fn, epochs=3, log_dir=\"logs/a\"):\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    with open(f\"{log_dir}/train.txt\", \"w\") as tf, open(f\"{log_dir}/val.txt\", \"w\") as vf:\n",
    "        tf.write(f\"Epoch,Loss,Score\\n\")\n",
    "        vf.write(\"Epoch,Loss,Score\\n\")\n",
    "    best_val_score = 0\n",
    "    \n",
    "    for epoch in range(1,1+epochs):\n",
    "        train_loss, tscore = train_on_batches(model, tdataloader, optimizer, loss_fn, grad_accum_bs=16)\n",
    "        val_loss, score = validation(model, vdataloader, loss_fn)\n",
    "        with open(f\"{log_dir}/train.txt\", \"a+\") as tf, open(f\"{log_dir}/val.txt\", \"a+\") as vf:\n",
    "            tf.write(f\"{epoch},{train_loss},{tscore}\\n\")\n",
    "            vf.write(f\"{epoch},{val_loss},{score}\\n\")\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        tqdm.write(f\"Epoch: {epoch} | LR: {lr:.7f} | Train Loss: {train_loss:.4f} | Train Score: {tscore:.4f} | Val Loss: {val_loss:.4f} | Val Score: {score:.4f}\")\n",
    "        \n",
    "        if score > best_val_score:\n",
    "            model.save_pretrained(log_dir+\"/model\")\n",
    "            best_val_score = score\n",
    "\n",
    "        # update the learning rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 1.0*float(param_group['lr'])\n",
    "\n",
    "        train_df = pd.read_csv(f\"{log_dir}/train.txt\")\n",
    "        val_df = pd.read_csv(f\"{log_dir}/val.txt\")\n",
    "\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(20,8))\n",
    "        ax[0].plot(range(1,len(train_df)+1), train_df[\"Loss\"], label=\"Train\")\n",
    "        ax[0].plot(range(1,len(val_df)+1), val_df[\"Loss\"], label=\"Val\")\n",
    "\n",
    "        ax[1].plot(range(1,len(train_df)+1), train_df[\"Score\"], label=\"Train\")\n",
    "        ax[1].plot(range(1,len(val_df)+1), val_df[\"Score\"], label=\"Val\")\n",
    "\n",
    "        ax[0].set_xlabel(\"Epochs\")\n",
    "        ax[1].set_xlabel(\"Epochs\")\n",
    "\n",
    "        ax[0].set_ylabel(\"Loss\")\n",
    "        ax[1].set_ylabel(\"Score\")\n",
    "\n",
    "        ax[0].legend()\n",
    "        ax[1].legend()\n",
    "\n",
    "        fig.suptitle('', fontsize=16)\n",
    "        fig.savefig(\"plt.png\")\n",
    "\n",
    "\n",
    "def validation(model, val_data_loader, loss_fn):\n",
    "    val_loss = 0\n",
    "    score = 0\n",
    "    num_correct = 0\n",
    "    pbar = tqdm(range(len(val_data_loader)), ncols=100)\n",
    "    option_ids = [tokenizer(o).input_ids[0] for o in [\"1\", \"2\", \"3\", \"4\", \"5\"]]\n",
    "    total_num = 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for i, batch in enumerate(val_data_loader):\n",
    "            with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "                logits = forward_pass(model, batch)\n",
    "                loss = calc_loss(loss_fn, logits, batch).item()\n",
    "            val_loss += (1/(i+1))*(loss - val_loss)\n",
    "            pred = (logits[:,batch[\"q_tokens\"].input_ids.size(1)-1,option_ids].argmax(dim=1) + 1).tolist()\n",
    "            target = val_data_loader.tokenizer.batch_decode(batch[\"a_tokens\"].input_ids[:,0], skip_special_tokens=True)\n",
    "            # print(pred, target)\n",
    "            for p,t in zip(pred, target):\n",
    "                total_num += 1\n",
    "                num_correct += (int(p) == int(t))\n",
    "                score = num_correct/total_num\n",
    "                \n",
    "            pbar.set_description(f\"Val Loss: {val_loss:.4f} Score: {score:.4f}\")\n",
    "            pbar.update(1)\n",
    "            \n",
    "        gen_tokens = model.generate(**batch[\"q_tokens\"].to(model.device), max_new_tokens=10)[:,batch[\"q_tokens\"].input_ids.size(1):]\n",
    "        gen_txt = val_data_loader.tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "        target_txt = val_data_loader.tokenizer.batch_decode(batch[\"a_tokens\"].input_ids, skip_special_tokens=True)\n",
    "        tqdm.write(\"Target: \" + \"\\n\" + \"\\n\".join(target_txt) + \"\\nGenerated: \\n \" + \"\\n\".join(gen_txt) +\"\\n\")\n",
    "        pbar.close()\n",
    "\n",
    "    return val_loss, score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = MyLLMDataloader(4, tokenizer, \"cleaned_TeleQnA_train_context_gte.json\", shuffle=True)\n",
    "# for item in train_data:\n",
    "#     print(tokenizer.decode(item['inp_ids'][3]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "torch.set_default_device(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)\n",
    "model.to(device)\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=64,  # reduce if running into out-of-memory issues\n",
    "    lora_alpha=16,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'dense'],\n",
    "    # modules_to_save=[\"lm_head\"],\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.eos_token_id, reduction='none').cuda()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 1e-4)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "\n",
    "# trainLoader = TrainDataLoader(2, tokenizer, topk=topk)\n",
    "trainLoader = MyLLMDataloader(1, tokenizer, \"cleaned_TeleQnA_train_context_gte.json\", shuffle=True)\n",
    "\n",
    "valLoader = MyLLMDataloader(1, tokenizer, \"questions_365_val.json\", val=True)\n",
    "scaler =  torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "train(model, trainLoader, valLoader,optimizer, loss_fn, epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
