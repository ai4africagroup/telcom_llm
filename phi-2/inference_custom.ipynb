{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "# train_data = MyLLMDataloader(4, tokenizer, \"cleaned_TeleQnA_train_context_gte.json\", shuffle=True)\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "from statistics import mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"questions_new_final_backup.json\", \"r\") as f:\n",
    "    test_data = f.read()\n",
    "\n",
    "orig_test_data=json.loads(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "prompt_q_without_contex_train= Template('''Instruct: $question\n",
    "$options\n",
    "$question\n",
    "''')\n",
    "\n",
    "\n",
    "prompt_without_contex_train= Template('''Instruct: $question\n",
    "Abbreviations: $abbreviation\n",
    "          \n",
    "Considering the following contexts:\n",
    "context 1: $context1\n",
    "context 2: $context2\n",
    "context 3: $context3      \n",
    "                                                                    \n",
    "$question\n",
    "$options\n",
    "Output: option ''')\n",
    "# prompt_without_context = f'Hello {planet}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_question(question):\n",
    "    for num in [14, 15, 16, 17, 18]:\n",
    "        question = question.replace(f\"[3GPP Release {num}]\", \"\")\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "BASE_MODEL_ID = \"logs/a/model\"\n",
    "device = \"cuda\"\n",
    "torch.set_default_device(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLLMDataloader:\n",
    "    def __init__(self, batch_size, tokenizer, data, shuffle = False, val= False):\n",
    "        ## initializations\n",
    "        self.batch_size  = batch_size\n",
    "        self.tokenizer  = tokenizer\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        with open(data, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "        self.all_examples = list(self.data.keys())\n",
    "        self.shuffle = shuffle\n",
    "        self.val = val\n",
    "        \n",
    "        self.n_data_points = math.ceil(len(self.data)/self.batch_size)\n",
    "        self.indices = [i for i in range(self.n_data_points)]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        ## this gets a batch \n",
    "        option_header = [\"option 1 \", \"option 2 \", \"option 3 \", \"option 4 \", \"option 5 \"]\n",
    "        batch_start_id = idx * self.batch_size\n",
    "        batch_end_id  = min(len(self.data), batch_start_id + self.batch_size) \n",
    "        batch = {\"question_context\":[], \"answer\":[]}\n",
    "        \n",
    "        for i in range(batch_start_id, batch_end_id):\n",
    "            example = self.data[self.all_examples[i]]\n",
    "            options = []\n",
    "            opts = []\n",
    "            for key in example.keys():\n",
    "                if key.startswith(\"opt\"):\n",
    "                    if example[key] == None:\n",
    "                        continue\n",
    "                    options.append(example[key])\n",
    "                    opts.append((example[key], key.split(\"option \")[1]))\n",
    "            \n",
    "            string_opts = ' '.join(opt[0] for opt in opts)\n",
    "            batch_prompts = []\n",
    "            option_maps = []\n",
    "         \n",
    "            if not (\"option\" in string_opts or \"above\" in string_opts):\n",
    "\n",
    "                    all_permutations = list(itertools.permutations(opts))\n",
    "                    all_permutations = random.sample(all_permutations, 20 if len(all_permutations)>20 else len(all_permutations))\n",
    "                    for option_set in all_permutations:\n",
    "                        option_map = []\n",
    "                        options_with_header   = []\n",
    "                        for z in range(len(option_set)):\n",
    "\n",
    "                            options_with_header.append(option_header[z] +option_set[z][0])\n",
    "\n",
    "                         \n",
    "                            option_map.append(int(option_set[z][1]))\n",
    "                        \n",
    "                        \n",
    "                        options_with_header = \"\\n\".join(options_with_header)\n",
    "\n",
    "                        prompt = prompt_without_contex_train.substitute(question = clean_question(example[\"question\"]),\\\n",
    "                        abbreviation='\\n'.join(example[\"abbreviation\"]), context1 = '\\n'.join(example[\"context_qwen2\"][:2]) , context2 = '\\n'.join(example[\"context_gle\"]), context3 = '\\n'.join(example[\"context_bm\"][:2]),\n",
    "                        options =options_with_header)\n",
    "                        batch_prompts.append(prompt)\n",
    "                        option_maps.append(option_map)\n",
    "\n",
    "\n",
    "            else:\n",
    "                options_with_header = [option_header[i] +options[i] for i in range(len(options)) ]\n",
    "               \n",
    "                options_with_header = \"\\n\".join(options_with_header)\n",
    "                prompt = prompt_without_contex_train.substitute(question = clean_question(example[\"question\"]),\\\n",
    "                abbreviation='\\n'.join(example[\"abbreviation\"]), context1 = '\\n'.join(example[\"context_qwen2\"][:2]) , context2 = '\\n'.join(example[\"context_gle\"]), context3 = '\\n'.join(example[\"context_bm\"][:2]),\n",
    "                options =options_with_header)\n",
    "                batch_prompts.append(prompt)\n",
    "                \n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "            batch[\"question_context\"] += batch_prompts\n",
    "\n",
    "            # batch[\"answer\"] += [answer]\n",
    "\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "        q_tokens = self.tokenizer(batch[\"question_context\"], padding=\"longest\", return_tensors=\"pt\")  \n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "        # a_tokens = self.tokenizer(batch[\"answer\"], padding=\"longest\", return_tensors=\"pt\")\n",
    "        tokens = q_tokens\n",
    "        attn_masks = q_tokens[\"attention_mask\"]\n",
    "        # attn_masks = torch.cat([q_tokens[\"attention_mask\"], a_tokens[\"attention_mask\"]], dim=1)\n",
    "        # loss_mask = torch.cat([torch.zeros_like(q_tokens[\"attention_mask\"]), a_tokens[\"attention_mask\"]], dim=1)[:,1:]\n",
    "   \n",
    "        result = {\n",
    "        \"inp_ids\":tokens[\"input_ids\"],\n",
    "        \"inp_mask\":attn_masks,## Causal Training\n",
    "        \"option_maps\": option_maps\n",
    "        }\n",
    "\n",
    "        # result[\"loss_mask\"] = loss_mask * result[\"out_mask\"]\n",
    "        # result[\"out_ids\"][:,:q_tokens[\"input_ids\"].size(1)-10] = self.tokenizer.eos_token_id\n",
    "\n",
    "        return result       \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    def __iter__(self):\n",
    "        self.idx = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.idx >= self.n_data_points:\n",
    "            self.idx = 0\n",
    "            raise StopIteration\n",
    "        temp_idx = self.indices[self.idx]\n",
    "        self.idx += 1\n",
    "        return self[temp_idx]\n",
    "             \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_data_points\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, batch):\n",
    "    inp_ids = batch[\"inp_ids\"].to(model.device)\n",
    "    attn_mask = batch[\"inp_mask\"].to(model.device)\n",
    "    result = model(input_ids=inp_ids, attention_mask=attn_mask)\n",
    "    logits = result.logits\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, testLoader):\n",
    "    my_ans = {\"Answer_ID\": []}\n",
    "    k  = 0 \n",
    "    model.eval()          \n",
    "    option_ids = [tokenizer(o).input_ids[0] for o in [\"1\", \"2\", \"3\", \"4\", \"5\"]]\n",
    "    pbar = tqdm(range(len(testLoader)), ncols=100)\n",
    "    for item in testLoader:\n",
    "  \n",
    "        # print(item[\"inp_ids\"])\n",
    "        # if int(tokenizer.decode(item[\"a_tokens\"].input_ids[:,0], skip_special_tokens=True)) ==0:\n",
    "        if len(item[\"option_maps\"]) >0:\n",
    "            # print('\\n'.join(tokenizer.batch_decode(item['inp_ids']))),\n",
    "            # first_half = item.copy()\n",
    "\n",
    "            # batch = first_half[\"inp_ids\"]\n",
    "\n",
    "\n",
    " \n",
    "            logits = []\n",
    "            batched_data = item.copy()\n",
    "            for batch, attn_data in zip(item[\"inp_ids\"], item[\"inp_mask\"]):\n",
    "                batched_data[\"inp_ids\"] = torch.unsqueeze(batch,0)\n",
    "                batched_data[\"inp_mask\"] = torch.unsqueeze(attn_data,0)\n",
    "\n",
    "\n",
    "              \n",
    "                # print(first_half[\"inp_ids\"].shape, second_half[\"inp_ids\"].shape)\n",
    "                with torch.inference_mode():\n",
    "                    with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "                        # gen_tokens = model.generate(inputs=item[\"inp_ids\"].to(device), max_new_tokens=1)\n",
    "\n",
    "                        logits1 = forward_pass(model, batched_data)\n",
    "                        # logits2 = forward_pass(model, second_half)\n",
    "                  \n",
    "                logits.append(logits1)\n",
    "            logits = torch.cat(logits,axis=0)\n",
    "            # print(logits.shape)\n",
    "            \n",
    "            preds =(logits[:, -1, option_ids ].argmax(dim=1) )\n",
    "\n",
    "            z = torch.tensor(item[\"option_maps\"])\n",
    "            preds = torch.mode(z.gather(1, preds.unsqueeze(1)).squeeze(1)).values\n",
    "        \n",
    "            my_ans[\"Answer_ID\"].append(preds.item())\n",
    "            # print(\"logits\", tokenizer.batch_decode(gen_tokens,  skip_special_tokens=True)[0] )\n",
    "            # print(tokenizer.decode(item[\"a_tokens\"].input_ids[:,0], skip_special_tokens=True))\n",
    "            # break\n",
    "        else:\n",
    "            with torch.inference_mode():\n",
    "                with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "                    # gen_tokens = model.generate(inputs=item[\"inp_ids\"].to(device), max_new_tokens=1)\n",
    "                    logits = forward_pass(model, item)\n",
    "                preds =(logits[:, -1, option_ids ].argmax(dim=1) +1)\n",
    "                my_ans[\"Answer_ID\"].append(preds.item())\n",
    "\n",
    "        pd.DataFrame(my_ans).to_csv(\"law_ans.csv\")\n",
    "        pbar.set_description(f\"Prediction: {preds}\")\n",
    "        pbar.update(1)\n",
    "    return my_ans\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
