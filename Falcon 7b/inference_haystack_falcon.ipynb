{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "\n",
    "from haystack.nodes import PromptNode, PromptTemplate\n",
    "\n",
    "from haystack.nodes import PromptNode\n",
    "from haystack.nodes.prompt.prompt_model import PromptModel\n",
    "\n",
    "from typing import Optional, Union, List, Dict\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    StoppingCriteriaList,\n",
    "    StoppingCriteria,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedTokenizerFast,\n",
    "    GenerationConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "\n",
    "\n",
    "from transformers.pipelines import get_task\n",
    "\n",
    "from haystack.modeling.utils import initialize_device_settings\n",
    "from haystack.nodes.prompt.invocation_layer import PromptModelInvocationLayer, TokenStreamingHandler\n",
    "from haystack.nodes.prompt.invocation_layer.handlers import DefaultTokenStreamingHandler, HFTokenStreamingHandler\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "from haystack import Document, Pipeline\n",
    "\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import json\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomHFLocalInvocationLayer(PromptModelInvocationLayer):\n",
    "    \"\"\"\n",
    "    A subclass of the PromptModelInvocationLayer class. It loads a pre-trained model from Hugging Face and\n",
    "    passes a prepared prompt into that model.\n",
    "\n",
    "    Note: kwargs other than init parameter names are ignored to enable reflective construction of the class,\n",
    "    as many variants of PromptModelInvocationLayer are possible and they may have different parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str = \"google/flan-t5-base\",\n",
    "        max_length: int = 100,\n",
    "        use_auth_token: Optional[Union[str, bool]] = None,\n",
    "        use_gpu: Optional[bool] = False,\n",
    "        devices: Optional[List[Union[str, torch.device]]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates an instance of HFLocalInvocationLayer used to invoke local Hugging Face models.\n",
    "\n",
    "        :param model_name_or_path: The name or path of the underlying model.\n",
    "        :param max_length: The maximum number of tokens the output text can have.\n",
    "        :param use_auth_token: The token to use as HTTP bearer authorization for remote files.\n",
    "        :param use_gpu: Whether to use GPU for inference.\n",
    "        :param device: The device to use for inference.\n",
    "        :param kwargs: Additional keyword arguments passed to the underlying model. Due to reflective construction of\n",
    "        all PromptModelInvocationLayer instances, this instance of HFLocalInvocationLayer might receive some unrelated\n",
    "        kwargs. Only kwargs relevant to the HFLocalInvocationLayer are considered. The list of supported kwargs\n",
    "        includes: task_name, trust_remote_code, revision, feature_extractor, tokenizer, config, use_fast, torch_dtype, device_map.\n",
    "        For more details about pipeline kwargs in general, see\n",
    "        Hugging Face [documentation](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.pipeline).\n",
    "\n",
    "        This layer supports two additional kwargs: generation_kwargs and model_max_length.\n",
    "\n",
    "        The generation_kwargs are used to customize text generation for the underlying pipeline. See Hugging\n",
    "        Face [docs](https://huggingface.co/docs/transformers/main/en/generation_strategies#customize-text-generation)\n",
    "        for more details.\n",
    "\n",
    "        The model_max_length is used to specify the custom sequence length for the underlying pipeline.\n",
    "        \"\"\"\n",
    "        super().__init__(model_name_or_path)\n",
    "        self.use_auth_token = use_auth_token\n",
    "\n",
    "        self.devices, _ = initialize_device_settings(devices=devices, use_cuda=use_gpu, multi_gpu=False)\n",
    "        if len(self.devices) > 1:\n",
    "            logger.warning(\n",
    "                \"Multiple devices are not supported in %s inference, using the first device %s.\",\n",
    "                self.__class__.__name__,\n",
    "                self.devices[0],\n",
    "            )\n",
    "\n",
    "        # Due to reflective construction of all invocation layers we might receive some\n",
    "        # unknown kwargs, so we need to take only the relevant.\n",
    "        # For more details refer to Hugging Face pipeline documentation\n",
    "        # Do not use `device_map` AND `device` at the same time as they will conflict\n",
    "        model_input_kwargs = {\n",
    "            key: kwargs[key]\n",
    "            for key in [\n",
    "                \"model_kwargs\",\n",
    "                \"trust_remote_code\",\n",
    "                \"revision\",\n",
    "                \"feature_extractor\",\n",
    "                \"tokenizer\",\n",
    "                \"config\",\n",
    "                \"use_fast\",\n",
    "                \"torch_dtype\",\n",
    "                \"device_map\",\n",
    "                \"generation_kwargs\",\n",
    "                \"model_max_length\",\n",
    "                \"stream\",\n",
    "                \"stream_handler\",\n",
    "            ]\n",
    "            if key in kwargs\n",
    "        }\n",
    "        # flatten model_kwargs one level\n",
    "        if \"model_kwargs\" in model_input_kwargs:\n",
    "            mkwargs = model_input_kwargs.pop(\"model_kwargs\")\n",
    "            model_input_kwargs.update(mkwargs)\n",
    "\n",
    "        # save stream settings and stream_handler for pipeline invocation\n",
    "        self.stream_handler = model_input_kwargs.pop(\"stream_handler\", None)\n",
    "        self.stream = model_input_kwargs.pop(\"stream\", False)\n",
    "\n",
    "        # save generation_kwargs for pipeline invocation\n",
    "        self.generation_kwargs = model_input_kwargs.pop(\"generation_kwargs\", {})\n",
    "        model_max_length = model_input_kwargs.pop(\"model_max_length\", None)\n",
    "\n",
    "        torch_dtype = model_input_kwargs.get(\"torch_dtype\")\n",
    "        if torch_dtype is not None:\n",
    "            if isinstance(torch_dtype, str):\n",
    "                if \"torch.\" in torch_dtype:\n",
    "                    torch_dtype_resolved = getattr(torch, torch_dtype.strip(\"torch.\"))\n",
    "                elif torch_dtype == \"auto\":\n",
    "                    torch_dtype_resolved = torch_dtype\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        f\"torch_dtype should be a torch.dtype, a string with 'torch.' prefix or the string 'auto', got {torch_dtype}\"\n",
    "                    )\n",
    "            elif isinstance(torch_dtype, torch.dtype):\n",
    "                torch_dtype_resolved = torch_dtype\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid torch_dtype value {torch_dtype}\")\n",
    "            model_input_kwargs[\"torch_dtype\"] = torch_dtype_resolved\n",
    "\n",
    "        if len(model_input_kwargs) > 0:\n",
    "            logger.info(\"Using model input kwargs %s in %s\", model_input_kwargs, self.__class__.__name__)\n",
    "\n",
    "        # If task_name is not provided, get the task name from the model name or path (uses HFApi)\n",
    "        if \"task_name\" in kwargs:\n",
    "            self.task_name = kwargs.get(\"task_name\")\n",
    "        else:\n",
    "            self.task_name = get_task(model_name_or_path, use_auth_token=use_auth_token)\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.pipe = pipeline(\n",
    "            task=self.task_name,  # task_name is used to determine the pipeline type\n",
    "            model=model_name_or_path,\n",
    "            tokenizer=tokenizer,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\", \n",
    "            model_kwargs=model_input_kwargs,           \n",
    "        )\n",
    "        # This is how the default max_length is determined for Text2TextGenerationPipeline shown here\n",
    "        # https://huggingface.co/transformers/v4.6.0/_modules/transformers/pipelines/text2text_generation.html\n",
    "        # max_length must be set otherwise HFLocalInvocationLayer._ensure_token_limit will fail.\n",
    "        self.max_length = max_length or self.pipe.model.config.max_length\n",
    "\n",
    "        # we allow users to override the tokenizer's model_max_length because models like T5 have relative positional\n",
    "        # embeddings and can accept sequences of more than 512 tokens\n",
    "        if model_max_length is not None:\n",
    "            self.pipe.tokenizer.model_max_length = model_max_length\n",
    "\n",
    "        if self.max_length > self.pipe.tokenizer.model_max_length:\n",
    "            logger.warning(\n",
    "                \"The max_length %s is greater than model_max_length %s. This might result in truncation of the \"\n",
    "                \"generated text. Please lower the max_length (number of answer tokens) parameter!\",\n",
    "                self.max_length,\n",
    "                self.pipe.tokenizer.model_max_length,\n",
    "            )\n",
    "\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        It takes a prompt and returns a list of generated texts using the local Hugging Face transformers model\n",
    "        :return: A list of generated texts.\n",
    "\n",
    "        Note: Only kwargs relevant to Text2TextGenerationPipeline and TextGenerationPipeline are passed to\n",
    "        Hugging Face as model_input_kwargs. Other kwargs are ignored.\n",
    "        \"\"\"\n",
    "        \n",
    "        output: List[Dict[str, str]] = []\n",
    "        stop_words = kwargs.pop(\"stop_words\", None)\n",
    "        top_k = kwargs.pop(\"top_k\", None)\n",
    "        # either stream is True (will use default handler) or stream_handler is provided for custom handler\n",
    "        stream = kwargs.get(\"stream\", self.stream) or kwargs.get(\"stream_handler\", self.stream_handler) is not None\n",
    "        if kwargs and \"prompt\" in kwargs:\n",
    "            prompt = kwargs.pop(\"prompt\")\n",
    "\n",
    "            # Consider only Text2TextGenerationPipeline and TextGenerationPipeline relevant, ignore others\n",
    "            # For more details refer to Hugging Face Text2TextGenerationPipeline and TextGenerationPipeline\n",
    "            # documentation\n",
    "            # TODO resolve these kwargs from the pipeline signature\n",
    "            model_input_kwargs = {\n",
    "                key: kwargs[key]\n",
    "                for key in [\n",
    "                    \"return_tensors\",\n",
    "                    \"return_text\",\n",
    "                    \"return_full_text\",\n",
    "                    \"clean_up_tokenization_spaces\",\n",
    "                    \"truncation\",\n",
    "                    \"generation_kwargs\",\n",
    "                    \"max_new_tokens\",\n",
    "                    \"num_beams\",\n",
    "                    \"do_sample\",\n",
    "                    \"num_return_sequences\",\n",
    "                    \"max_length\",\n",
    "                ]\n",
    "                if key in kwargs\n",
    "            }\n",
    "            generation_kwargs = model_input_kwargs.pop(\"generation_kwargs\", self.generation_kwargs)\n",
    "            if isinstance(generation_kwargs, dict):\n",
    "                model_input_kwargs.update(generation_kwargs)\n",
    "            elif isinstance(generation_kwargs, GenerationConfig):\n",
    "                gen_dict = generation_kwargs.to_diff_dict()\n",
    "                gen_dict.pop(\"transformers_version\", None)\n",
    "                model_input_kwargs.update(gen_dict)\n",
    "\n",
    "            is_text_generation = \"text-generation\" == self.task_name\n",
    "            # Prefer return_full_text is False for text-generation (unless explicitly set)\n",
    "            # Thus only generated text is returned (excluding prompt)\n",
    "            if is_text_generation and \"return_full_text\" not in model_input_kwargs:\n",
    "                model_input_kwargs[\"return_full_text\"] = False\n",
    "                model_input_kwargs[\"max_new_tokens\"] = self.max_length\n",
    "            if stop_words:\n",
    "                sw = StopWordsCriteria(tokenizer=self.pipe.tokenizer, stop_words=stop_words, device=self.pipe.device)\n",
    "                model_input_kwargs[\"stopping_criteria\"] = StoppingCriteriaList([sw])\n",
    "            if top_k:\n",
    "                model_input_kwargs[\"num_return_sequences\"] = top_k\n",
    "                if \"num_beams\" not in model_input_kwargs or model_input_kwargs[\"num_beams\"] < top_k:\n",
    "                    if \"num_beams\" in model_input_kwargs:\n",
    "                        logger.warning(\"num_beams should not be less than top_k, hence setting it to %s\", top_k)\n",
    "                    model_input_kwargs[\"num_beams\"] = top_k\n",
    "            # max_new_tokens is used for text-generation and max_length for text2text-generation\n",
    "            if is_text_generation:\n",
    "                model_input_kwargs[\"max_new_tokens\"] = model_input_kwargs.pop(\"max_length\", self.max_length)\n",
    "            else:\n",
    "                model_input_kwargs[\"max_length\"] = self.max_length\n",
    "\n",
    "            if stream:\n",
    "                stream_handler: TokenStreamingHandler = kwargs.pop(\"stream_handler\", DefaultTokenStreamingHandler())\n",
    "                model_input_kwargs[\"streamer\"] = HFTokenStreamingHandler(self.pipe.tokenizer, stream_handler)\n",
    "            \n",
    "            output = self.pipe(prompt, **model_input_kwargs)\n",
    "            \n",
    "        generated_texts = [o[\"generated_text\"] for o in output if \"generated_text\" in o]\n",
    "\n",
    "        if stop_words:\n",
    "            # Although HF generates text until stop words are encountered unfortunately it includes the stop word\n",
    "            # We want to exclude it to be consistent with other invocation layers\n",
    "            for idx, _ in enumerate(generated_texts):\n",
    "                for stop_word in stop_words:\n",
    "                    generated_texts[idx] = generated_texts[idx].replace(stop_word, \"\").strip()\n",
    "        return generated_texts\n",
    "\n",
    "    def _ensure_token_limit(self, prompt: Union[str, List[Dict[str, str]]]) -> Union[str, List[Dict[str, str]]]:\n",
    "        \"\"\"Ensure that the length of the prompt and answer is within the max tokens limit of the model.\n",
    "        If needed, truncate the prompt text so that it fits within the limit.\n",
    "\n",
    "        :param prompt: Prompt text to be sent to the generative model.\n",
    "        \"\"\"\n",
    "        model_max_length = self.pipe.tokenizer.model_max_length\n",
    "        n_prompt_tokens = len(self.pipe.tokenizer.tokenize(prompt))\n",
    "        n_answer_tokens = self.max_length\n",
    "        if (n_prompt_tokens + n_answer_tokens) <= model_max_length:\n",
    "            return prompt\n",
    "\n",
    "        logger.warning(\n",
    "            \"The prompt has been truncated from %s tokens to %s tokens so that the prompt length and \"\n",
    "            \"answer length (%s tokens) fit within the max token limit (%s tokens). \"\n",
    "            \"Shorten the prompt to prevent it from being cut off\",\n",
    "            n_prompt_tokens,\n",
    "            max(0, model_max_length - n_answer_tokens),\n",
    "            n_answer_tokens,\n",
    "            model_max_length,\n",
    "        )\n",
    "\n",
    "        tokenized_payload = self.pipe.tokenizer.tokenize(prompt)\n",
    "        decoded_string = self.pipe.tokenizer.convert_tokens_to_string(\n",
    "            tokenized_payload[: model_max_length - n_answer_tokens]\n",
    "        )\n",
    "        return decoded_string\n",
    "\n",
    "    @classmethod\n",
    "    def supports(cls, model_name_or_path: str, **kwargs) -> bool:\n",
    "        task_name: Optional[str] = None\n",
    "        if os.path.exists(model_name_or_path):\n",
    "            return True\n",
    "\n",
    "        try:\n",
    "            task_name = get_task(model_name_or_path, use_auth_token=kwargs.get(\"use_auth_token\", None))\n",
    "        except RuntimeError:\n",
    "            # This will fail for all non-HF models\n",
    "            return False\n",
    "        # if we are using an api_key it could be HF inference point\n",
    "        using_api_key = kwargs.get(\"api_key\", None) is not None\n",
    "        return not using_api_key and task_name in [\"text2text-generation\", \"text-generation\"]\n",
    "\n",
    "\n",
    "class StopWordsCriteria(StoppingCriteria):\n",
    "    \"\"\"\n",
    "    Stops text generation if any one of the stop words is generated.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],\n",
    "        stop_words: List[str],\n",
    "        device: Union[str, torch.device] = \"cpu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.stop_words = tokenizer(stop_words, add_special_tokens=False, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        return any(torch.isin(input_ids[-1], self.stop_words[\"input_ids\"]))\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"Loading model\")\n",
    "   # Create an instance of HFLocalInvocationLayer with trust_remote_code=True\n",
    "    \n",
    "    falcon_prompt_model = PromptModel(\n",
    "        model_name_or_path=\"hipnologo/falcon-7b-qlora-finetune-chatbot\", # Specify the model path here\n",
    "         model_kwargs={\"task_name\": \"text-generation\"},\n",
    "        invocation_layer_class=CustomHFLocalInvocationLayer\n",
    "    )\n",
    "  \n",
    "    print(\"Try loading PromptNode\")\n",
    "    prompt_node = PromptNode(\n",
    "        model_name_or_path=falcon_prompt_model,\n",
    "     \n",
    "    )\n",
    "    answer=prompt_node(\"What is the capital of Germany?\")\n",
    "    print(answer)\n",
    "\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_ans  = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5}\n",
    "with open(\"/home/admin/blessed/questions_new_final.json\", \"r\") as f:\n",
    "    test_data_ans = f.read()\n",
    "\n",
    "submission = {\"Question_ID\": [], \"Answer_ID\": []}\n",
    "test_data_ans = json.loads(test_data_ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "i = 0\n",
    "\n",
    "for ex in test_data_ans:\n",
    "    question  = test_data_ans[ex][\"question\"]\n",
    "    options = \"\\n\"\n",
    "    for key in test_data_ans[ex].keys():\n",
    "        if key.startswith(\"opt\"):\n",
    "            if test_data_ans[ex][key] == None:\n",
    "                continue\n",
    "            options += str(key) + \" \" +test_data_ans[ex][key]+\"\\n\"\n",
    "\n",
    "    # imput = \"Youre a Telecommunication standards expert. Please answer question by selecting the most likely option (1-5), only output the correct option, first consider the given context for the answer \\n\" + question+ \"\\n\\nConsidering the following retrieved contexts\"+\"\\ncontext 1: \"+ \"\\n\".join(test_data_ans[ex][\"context_bm_300\"])+ \"\\ncontext 2: \"+\"\\n\".join(test_data_ans[ex][\"context_gte\"][:5])+ \"\\n\" + question+ options \n",
    "    imput = \"Youre a Telecommunication standards expert. Please answer the question  first consider the given context for the answer \\n\" + question+ \"\\n\\nConsidering the following retrieved contexts\"+ \"\\ncontext 1: \"+ \"\\n\".join(test_data_ans[ex][\"context_gte\"][:2])+\"\\ncontext 2: \"+ \"\\n\".join(test_data_ans[ex][\"context_bm_300\"][:2]) +\"\\n\".join(test_data_ans[ex][\"context_gle\"][:5])+ \"\\n\" + question\n",
    "    # print(imput)\n",
    "\n",
    "    answer = prompt_node(imput)\n",
    "\n",
    "    # print(answer)\n",
    "\n",
    "    # try:\n",
    "    #     submission[\"Answer_ID\"].append(map_ans[answer[0].split(\"\\n\\nAnswer: \")[1][0]])\n",
    "    # except:\n",
    "    #     try:\n",
    "    #         submission[\"Answer_ID\"].append(map_ans[answer])\n",
    "    #     except:\n",
    "            # print(\"bad\", answer, type(answer))\n",
    "    submission[\"Answer_ID\"].append(answer)\n",
    "    submission[\"Question_ID\"].append(int(ex.split(\"question \")[1]))\n",
    "    i +=1\n",
    "    if i == 365:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_submission = pd.DataFrame(submission)\n",
    "df_submission.to_csv(\"submission_falcon.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(submission[\"Answer_ID\"])):\n",
    "    print(i)\n",
    "    if isinstance(submission[\"Answer_ID\"][i], list):\n",
    "        query = submission[\"Answer_ID\"][i]\n",
    "        q_id = \"question \"+ str(submission[\"Question_ID\"][i])\n",
    "        opt = []\n",
    "        for key in test_data_ans[q_id]:\n",
    "            if key.startswith(\"option\"):\n",
    "                opt.append(test_data_ans[q_id][key])\n",
    "        embedding  = model.encode(query + opt )\n",
    "        scores = (embedding[1:]@embedding[0]).argmax()  + 1\n",
    "        submission[\"Answer_ID\"][i] = scores\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_submission = pd.DataFrame(submission)\n",
    "df_submission.to_csv(\"submission_falcon.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
